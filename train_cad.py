"""
Fine-tuning script for Compressed Algorithm Distillation (CAD).

This script fine-tunes the full CAD system (compression + AD) after pre-training
the compression transformer. Supports multi-GPU training and curriculum learning.

Usage:
    accelerate launch train_cad.py
    
For multi-GPU:
    accelerate launch --multi_gpu --num_processes=N train_cad.py
    
With config:
    accelerate config  # First time setup
    accelerate launch train_cad.py
"""

from datetime import datetime
import os
import os.path as path
from glob import glob
import argparse
import gc

import yaml
from accelerate import Accelerator
from accelerate.utils import set_seed

import yaml
import torch
from torch.optim import AdamW
from torch.utils.tensorboard import SummaryWriter

from dataset import CompressedADDataset, ADDataset
from env import SAMPLE_ENVIRONMENT
from model import MODEL
from utils import get_config, next_dataloader, get_curriculum_aware_scheduler
from transformers import get_cosine_schedule_with_warmup

import multiprocessing
from tqdm import tqdm
from stable_baselines3.common.vec_env import SubprocVecEnv

from env import make_env
import numpy as np
import torch.nn.functional as F
from functools import partial
import random


# Global config that collate function can access
# This is updated by the main training loop when curriculum changes
_collate_config = {
    'n_transit': 60,
    'min_context': 50,
    'max_context': 800,
    'length_distribution': {'short': 0.6, 'medium': 0.35, 'long': 0.05, 'very_long': 0.0}
}


def update_collate_config(n_transit, min_context, max_context, length_distribution):
    """Update the global collate config (called from main training loop)."""
    global _collate_config
    _collate_config['n_transit'] = n_transit
    _collate_config['min_context'] = min_context
    _collate_config['max_context'] = max_context
    _collate_config['length_distribution'] = length_distribution.copy()


def _sample_batch_context_length():
    """Sample a context length for the entire batch based on current distribution."""
    cfg = _collate_config
    r = random.random()
    cumulative = 0
    
    compression_trigger = cfg['n_transit'] - 1
    
    for category, prob in cfg['length_distribution'].items():
        cumulative += prob
        if r < cumulative:
            if category == 'short':
                low = max(20, cfg['min_context'])
                high = min(compression_trigger - 10, compression_trigger - 1)
            elif category == 'medium':
                low = compression_trigger
                high = min(int(compression_trigger * 2.5), cfg['max_context'])
            elif category == 'long':
                low = min(int(compression_trigger * 2.5), cfg['max_context'])
                high = min(int(compression_trigger * 5), cfg['max_context'])
            elif category == 'very_long':
                low = min(int(compression_trigger * 5), cfg['max_context'])
                high = cfg['max_context']
            else:
                low, high = cfg['min_context'], cfg['max_context']
            
            low = min(low, high)
            return random.randint(low, high)
    
    return random.randint(cfg['min_context'], cfg['max_context'])


def cad_collate_fn(batch, grid_size):
    """
    Collate function for CAD dataset.
    
    Samples ONE context length for the entire batch, ensuring:
    1. All samples in batch have same length (efficient batched processing)
    2. The curriculum length distribution is properly respected
    3. Model sees full variety of sequence lengths across training
    
    IMPORTANT: When truncating, we keep the MOST RECENT context (closest to query).
    The query state is the state we're predicting action for, so recent context is most relevant.
    """
    batch_size = len(batch)
    dim_state = batch[0]['states'].shape[1]
    num_actions = 5  # Darkroom actions
    
    # Sample a single context length for this entire batch
    shared_context_len = _sample_batch_context_length()
    
    # Initialize arrays with uniform length
    states = np.zeros((batch_size, shared_context_len, dim_state), dtype=np.float32)
    actions = np.zeros((batch_size, shared_context_len), dtype=np.int64)
    rewards = np.zeros((batch_size, shared_context_len), dtype=np.float32)
    next_states = np.zeros((batch_size, shared_context_len, dim_state), dtype=np.float32)
    
    query_states = []
    target_actions = []
    
    for i, item in enumerate(batch):
        orig_len = item['states'].shape[0]
        
        if orig_len >= shared_context_len:
            # Truncate: keep the MOST RECENT context (last shared_context_len transitions)
            # These are the transitions immediately before the query state
            start_idx = orig_len - shared_context_len
            states[i] = item['states'][start_idx:]
            actions[i] = item['actions'][start_idx:]
            rewards[i] = item['rewards'][start_idx:]
            next_states[i] = item['next_states'][start_idx:]
        else:
            # Item is shorter than needed - use all of it, pad at the BEGINNING
            # Padding at beginning = "no history before this point"
            pad_len = shared_context_len - orig_len
            states[i, pad_len:] = item['states']
            actions[i, pad_len:] = item['actions']
            rewards[i, pad_len:] = item['rewards']
            next_states[i, pad_len:] = item['next_states']
        
        query_states.append(item['query_states'])
        target_actions.append(item['target_actions'])
    
    res = {
        'query_states': torch.tensor(np.array(query_states), requires_grad=False, dtype=torch.float),
        'target_actions': torch.tensor(np.array(target_actions), requires_grad=False, dtype=torch.long),
        'states': torch.tensor(states, requires_grad=False, dtype=torch.float),
        'actions': F.one_hot(torch.tensor(actions, requires_grad=False, dtype=torch.long), num_classes=num_actions),
        'rewards': torch.tensor(rewards, dtype=torch.float, requires_grad=False),
        'next_states': torch.tensor(next_states, requires_grad=False, dtype=torch.float),
    }
    
    return res


def get_cad_data_loader(dataset, batch_size, config, shuffle=True):
    """Data loader for CAD with batch-shared context length."""
    from torch.utils.data import DataLoader
    
    # Initialize global collate config
    update_collate_config(
        n_transit=config['n_transit'],
        min_context=config.get('min_context_length', 50),
        max_context=config.get('max_context_length', 800),
        length_distribution=dataset.length_distribution
    )
    
    collate_fn = partial(cad_collate_fn, grid_size=config['grid_size'])
    return DataLoader(
        dataset, 
        batch_size=batch_size, 
        shuffle=shuffle, 
        collate_fn=collate_fn, 
        num_workers=config['num_workers'], 
        persistent_workers=True
    )


# Curriculum schedule: (step, max_compressions)
DEFAULT_CURRICULUM = [
    (0, 1),       # Start with max 1 compression
    (10000, 2),   # Allow 2 compressions
    (25000, 3),   # Allow 3 compressions
    (40000, None), # Unlimited
]

# Default length distributions for each curriculum stage
DEFAULT_LENGTH_DISTRIBUTIONS = {
    1: {'short': 0.50, 'medium': 0.45, 'long': 0.05, 'very_long': 0.00},
    2: {'short': 0.35, 'medium': 0.40, 'long': 0.20, 'very_long': 0.05},
    3: {'short': 0.25, 'medium': 0.30, 'long': 0.30, 'very_long': 0.15},
    None: {'short': 0.20, 'medium': 0.25, 'long': 0.30, 'very_long': 0.25},
}


def get_curriculum_from_config(config):
    """
    Get curriculum schedule from config or use default.
    
    Returns:
        list of tuples: [(step, max_compressions, length_distribution), ...]
    """
    if 'curriculum_schedule' in config:
        curriculum = []
        for item in config['curriculum_schedule']:
            step = item['step']
            max_comp = item['max_compressions']
            # Get length distribution from config or use default
            length_dist = item.get('length_distribution', 
                                   DEFAULT_LENGTH_DISTRIBUTIONS.get(max_comp, DEFAULT_LENGTH_DISTRIBUTIONS[None]))
            curriculum.append((step, max_comp, length_dist))
        return curriculum
    # Default curriculum with default distributions
    return [(0, 1, DEFAULT_LENGTH_DISTRIBUTIONS[1]),
            (10000, 2, DEFAULT_LENGTH_DISTRIBUTIONS[2]),
            (25000, 3, DEFAULT_LENGTH_DISTRIBUTIONS[3]),
            (40000, None, DEFAULT_LENGTH_DISTRIBUTIONS[None])]


def get_curriculum_max_compressions(step, curriculum):
    """Get max compressions allowed at current step."""
    max_comp = curriculum[0][1]
    for threshold, comp, _ in curriculum:
        if step >= threshold:
            max_comp = comp
    return max_comp


def get_curriculum_length_distribution(step, curriculum):
    """Get length distribution for current curriculum stage."""
    length_dist = curriculum[0][2]
    for threshold, _, dist in curriculum:
        if step >= threshold:
            length_dist = dist
    return length_dist


def get_curriculum_stage(step, curriculum):
    """Get the current curriculum stage index."""
    stage = 0
    for i, (threshold, _, _) in enumerate(curriculum):
        if step >= threshold:
            stage = i
    return stage


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--pretrain_ckpt', type=str, default=None,
                       help='Path to pre-trained compression checkpoint')
    parser.add_argument('--no_curriculum', action='store_true',
                       help='Disable curriculum learning')
    parser.add_argument('--config', type=str, default='cad_dr',
                       help='Model config name (without .yaml extension)')
    parser.add_argument('--env', type=str, default='darkroom',
                       help='Environment name: darkroom or dark_key_to_door')
    args = parser.parse_args()
    
    multiprocessing.set_start_method('spawn', force=True)
    
    # Determine config files based on environment
    env_config_map = {
        'darkroom': ('darkroom', 'ppo_darkroom'),
        'dark_key_to_door': ('dark_key_to_door', 'ppo_dark_key_to_door'),
    }
    if args.env not in env_config_map:
        raise ValueError(f'Unknown environment: {args.env}')
    env_cfg, alg_cfg = env_config_map[args.env]
    
    # Load configs
    config = get_config(f'./config/env/{env_cfg}.yaml')
    config.update(get_config(f'./config/algorithm/{alg_cfg}.yaml'))
    config.update(get_config(f'./config/model/{args.config}.yaml'))

    # Set seed for reproducibility
    set_seed(config.get('seed', 42))

    log_dir = path.join('./runs', f"CAD-{config['env']}-seed{config['env_split_seed']}")
    
    # Check if already exists
    config_save_path = path.join(log_dir, 'config.yaml')
    try:
        with open(config_save_path, 'r') as f:
            f.read(1)
            config_exists = True
    except FileNotFoundError:
        config_exists = False

    if config_exists:
        print(f'WARNING: {log_dir} already exists. Will resume if checkpoint exists.')
    
    config['log_dir'] = log_dir
    config['traj_dir'] = './datasets'
    config['mixed_precision'] = 'fp16'

    # Curriculum settings - load from config or use default
    use_curriculum = not args.no_curriculum
    curriculum = get_curriculum_from_config(config) if use_curriculum else [(0, None, DEFAULT_LENGTH_DISTRIBUTIONS[None])]

    # Initialize accelerator for multi-GPU support
    # Note: find_unused_parameters=True is needed because compression_transformer
    # and reconstruction_decoder are not used when sequences are short (no compression)
    from accelerate import DistributedDataParallelKwargs
    ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)
    
    accelerator = Accelerator(
        mixed_precision=config['mixed_precision'],
        gradient_accumulation_steps=config.get('gradient_accumulation_steps', 1),
        kwargs_handlers=[ddp_kwargs],
    )
    
    config['device'] = accelerator.device
    
    # Only main process prints and logs
    is_main = accelerator.is_main_process
    
    if is_main:
        os.makedirs(log_dir, exist_ok=True)
        writer = SummaryWriter(log_dir, flush_secs=15)
        print(f'Using Device: {config["device"]}')
        print(f'Number of processes: {accelerator.num_processes}')
        print(f'Curriculum enabled: {use_curriculum}')
        print(f'Curriculum schedule: {curriculum}')
        print(f'Max context length: {config.get("max_context_length", 800)}')
        print(f'Train source timesteps: {config.get("train_source_timesteps", 1000)}')
        print(f'Train timesteps: {config.get("train_timesteps", 100000)}')
        
        # Save config
        config_save_path = path.join(log_dir, 'config.yaml')
        with open(config_save_path, 'w') as f:
            yaml.dump(config, f)

    # Create model
    model = MODEL[config['model']](config)

    # Load pre-trained compression if available
    if args.pretrain_ckpt:
        if is_main:
            print(f'Loading pre-trained compression from {args.pretrain_ckpt}')
        model.load_pretrained_compression(args.pretrain_ckpt)
    else:
        # Try to find pre-trained checkpoint automatically
        pretrain_dir = path.join('./runs', f"CAD-pretrain-{config['env']}-seed{config['env_split_seed']}")
        pretrain_path = path.join(pretrain_dir, 'pretrain-final.pt')
        if path.exists(pretrain_path):
            if is_main:
                print(f'Found pre-trained compression at {pretrain_path}')
            model.load_pretrained_compression(pretrain_path)
        elif is_main:
            print('WARNING: No pre-trained compression found. Training from scratch.')

    if is_main:
        load_start_time = datetime.now()
        print(f'Data loading started at {load_start_time}')

    # Create datasets
    train_dataset = CompressedADDataset(
        config, 
        config['traj_dir'], 
        'train', 
        config['train_n_stream'], 
        config['train_source_timesteps']
    )
    
    if is_main:
        print(f'Dataset sequence length: {train_dataset.seq_length}')
        print(f'Number of training histories: {train_dataset.n_histories}')
    
    # Use standard AD dataset for testing (fixed length)
    test_dataset = ADDataset(
        config, 
        config['traj_dir'], 
        'test', 
        1, 
        config['train_source_timesteps']
    )

    train_dataloader = get_cad_data_loader(
        train_dataset, 
        batch_size=config['train_batch_size'], 
        config=config, 
        shuffle=True
    )
    train_dataloader = next_dataloader(train_dataloader)

    # Standard data loader for test - use fewer workers and no persistence to avoid leaks
    from torch.utils.data import DataLoader
    from functools import partial
    from utils import ad_collate_fn
    
    test_collate_fn = partial(ad_collate_fn, grid_size=config['grid_size'])
    test_dataloader = DataLoader(
        test_dataset, 
        batch_size=config['test_batch_size'], 
        shuffle=False,
        collate_fn=test_collate_fn,
        num_workers=0,  # Use main process to avoid worker issues during evaluation
        persistent_workers=False
    )
    
    if is_main:
        load_end_time = datetime.now()
        print(f'Data loading ended at {load_end_time}')
        print(f'Elapsed time: {load_end_time - load_start_time}')

    # Optimizer for all parameters
    optimizer = AdamW(
        model.parameters(), 
        lr=config['lr'], 
        betas=(config['beta1'], config['beta2']), 
        weight_decay=config['weight_decay']
    )
    
    # Use curriculum-aware scheduler if curriculum is enabled, otherwise use standard cosine
    if use_curriculum:
        lr_sched = get_curriculum_aware_scheduler(
            optimizer=optimizer,
            curriculum=curriculum,
            total_steps=config['train_timesteps'],
            initial_warmup_steps=config.get('num_warmup_steps', 1000),
            stage_warmup_steps=config.get('stage_warmup_steps', 500),
            min_lr_ratio=config.get('min_lr_ratio', 0.1),
        )
        if is_main:
            print(f'Using curriculum-aware LR scheduler with stage warmups')
    else:
        lr_sched = get_cosine_schedule_with_warmup(
            optimizer, 
            config['num_warmup_steps'], 
            config['train_timesteps']
        )
        if is_main:
            print(f'Using standard cosine LR scheduler')
    
    step = 0

    # Load checkpoint if exists
    ckpt_paths = sorted(glob(path.join(config['log_dir'], 'ckpt-*.pt')))
    if len(ckpt_paths) > 0:
        ckpt_path = ckpt_paths[-1]
        ckpt = torch.load(ckpt_path, map_location=config['device'])
        model.load_state_dict(ckpt['model'])
        optimizer.load_state_dict(ckpt['optimizer'])
        lr_sched.load_state_dict(ckpt['lr_sched'])
        step = ckpt['step']
        if is_main:
            print(f'Checkpoint loaded from {ckpt_path}')

    # Setup evaluation environments
    env_name = config['env']
    train_env_args, test_env_args = SAMPLE_ENVIRONMENT[env_name](config)
    train_env_args = train_env_args[:10]
    test_env_args = test_env_args[:10]
    env_args = train_env_args + test_env_args    

    if env_name == "darkroom":
        envs = SubprocVecEnv([make_env(config, goal=arg) for arg in env_args])
    elif env_name == "dark_key_to_door":
        envs = SubprocVecEnv([make_env(config, key=arg[:2], goal=arg[2:]) for arg in env_args])
    else:
        raise NotImplementedError(f'Environment not supported: {env_name}')

    # Prepare for distributed training
    model, optimizer, train_dataloader, lr_sched = accelerator.prepare(
        model, optimizer, train_dataloader, lr_sched
    )

    if is_main:
        start_time = datetime.now()
        print(f'Training started at {start_time}')

    # Track compression statistics
    compression_counts = []
    
    # Track current curriculum stage for length distribution updates
    current_curriculum_stage = -1
    
    # Best model tracking for early stopping / model selection
    best_eval_reward = -float('inf')
    best_step = 0
    patience_counter = 0
    patience = config.get('early_stopping_patience', 5)  # Number of eval intervals without improvement
    save_best_model = config.get('save_best_model', True)

    # Training loop
    with tqdm(total=config['train_timesteps'], position=0, leave=True, disable=not is_main) as pbar:
        pbar.update(step)

        while step < config['train_timesteps']:
            batch = next(train_dataloader)
            
            step += 1
            
            # Update curriculum (model max_compressions AND dataset length distribution)
            if use_curriculum:
                max_comp = get_curriculum_max_compressions(step, curriculum)
                unwrapped = accelerator.unwrap_model(model)
                unwrapped.set_curriculum(max_comp)
                
                # Check if curriculum stage changed - update dataset length distribution
                new_stage = get_curriculum_stage(step, curriculum)
                if new_stage != current_curriculum_stage:
                    current_curriculum_stage = new_stage
                    new_length_dist = get_curriculum_length_distribution(step, curriculum)
                    train_dataset.update_length_distribution(new_length_dist)
                    # Also update the global collate config for batch-level length sampling
                    update_collate_config(
                        n_transit=config['n_transit'],
                        min_context=config.get('min_context_length', 50),
                        max_context=config.get('max_context_length', 800),
                        length_distribution=new_length_dist
                    )
                    if is_main:
                        print(f'\n[Step {step}] Curriculum stage {new_stage}: max_comp={max_comp}, '
                              f'length_dist={new_length_dist}')
            
            with accelerator.autocast():
                output = model(batch)
            
            # Use total loss (action + reconstruction regularization)
            loss = output['loss_total']
            
            # Track compressions
            compression_counts.append(output['num_compressions'])
            if len(compression_counts) > 1000:
                compression_counts.pop(0)

            optimizer.zero_grad()
            accelerator.backward(loss)
            accelerator.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            if not accelerator.optimizer_step_was_skipped:
                lr_sched.step()

            avg_compressions = np.mean(compression_counts) if compression_counts else 0
            pbar.set_postfix(
                loss=loss.item(), 
                acc=output['acc_action'].item(),
                n_comp=output['num_compressions'],
                avg_comp=f'{avg_compressions:.2f}'
            )

            # Logging
            if is_main and step % config['summary_interval'] == 0:
                writer.add_scalar('train/loss', loss.item(), step)
                writer.add_scalar('train/loss_action', output['loss_action'].item(), step)
                writer.add_scalar('train/loss_recon', output['loss_recon'].item(), step)
                writer.add_scalar('train/lr', lr_sched.get_last_lr()[0], step)
                writer.add_scalar('train/acc_action', output['acc_action'].item(), step)
                writer.add_scalar('train/num_compressions', output['num_compressions'], step)
                writer.add_scalar('train/avg_compressions', avg_compressions, step)
                
                if use_curriculum:
                    curr_max = get_curriculum_max_compressions(step, curriculum)
                    writer.add_scalar('train/curriculum_max_compressions', 
                                    curr_max if curr_max is not None else -1, step)
                    writer.add_scalar('train/curriculum_stage', current_curriculum_stage, step)
                    
                    # Log length distribution
                    curr_dist = get_curriculum_length_distribution(step, curriculum)
                    for category, prob in curr_dist.items():
                        writer.add_scalar(f'train/length_dist_{category}', prob, step)

            # Evaluation
            if is_main and step % config['eval_interval'] == 0:
                torch.cuda.empty_cache()
                model.eval()
                eval_start_time = datetime.now()
                print(f'\nEvaluating started at {eval_start_time}')

                with torch.no_grad():
                    test_loss_action = 0.0
                    test_acc_action = 0.0
                    test_cnt = 0

                    for j, test_batch in enumerate(test_dataloader):
                        test_output = model(test_batch)
                        cnt = len(test_batch['states'])
                        test_loss_action += test_output['loss_action'].item() * cnt
                        test_acc_action += test_output['acc_action'].item() * cnt
                        test_cnt += cnt

                writer.add_scalar('test/loss_action', test_loss_action / test_cnt, step)
                writer.add_scalar('test/acc_action', test_acc_action / test_cnt, step)

                eval_end_time = datetime.now()
                print(f'Evaluating ended at {eval_end_time}')
                print(f'Elapsed time: {eval_end_time - eval_start_time}')
                
                # Clean up evaluation tensors
                del test_output, test_batch
                
                model.train()
                torch.cuda.empty_cache()
                gc.collect()

            # In-context evaluation (less frequent)
            if is_main and step % config['gen_interval'] == 0:
                torch.cuda.empty_cache()
                model.eval()
                
                with torch.no_grad():
                    unwrapped = accelerator.unwrap_model(model)
                    eval_output = unwrapped.evaluate_in_context(
                        vec_env=envs, 
                        eval_timesteps=config['horizon'] * 100
                    )
                    
                    mean_reward = eval_output['reward_episode'].mean()
                    total_compressions = eval_output['total_compressions']
                    
                    # Per-environment rewards for detailed tracking
                    env_rewards = eval_output['reward_episode'].mean(axis=1)
                    
                    writer.add_scalar('eval/mean_reward', mean_reward, step)
                    writer.add_scalar('eval/total_compressions', total_compressions, step)
                    for env_idx, env_reward in enumerate(env_rewards):
                        writer.add_scalar(f'eval/env_{env_idx}_reward', env_reward, step)
                    
                    print(f'\nIn-context eval: mean_reward={mean_reward:.3f}, compressions={total_compressions}')
                    print(f'Per-env rewards: {env_rewards}')
                    
                    # Best model tracking
                    if save_best_model and mean_reward > best_eval_reward:
                        best_eval_reward = mean_reward
                        best_step = step
                        patience_counter = 0
                        
                        # Save best model
                        best_ckpt_path = path.join(config['log_dir'], 'best-model.pt')
                        torch.save({
                            'step': step,
                            'config': config,
                            'model': unwrapped.state_dict(),
                            'optimizer': optimizer.state_dict(),
                            'lr_sched': lr_sched.state_dict(),
                            'eval_reward': mean_reward,
                        }, best_ckpt_path)
                        print(f'New best model saved! reward={mean_reward:.3f} at step {step}')
                    else:
                        patience_counter += 1
                        print(f'No improvement. Best: {best_eval_reward:.3f} at step {best_step} (patience: {patience_counter}/{patience})')
                    
                    del eval_output
                
                model.train()
                torch.cuda.empty_cache()
                gc.collect()

            pbar.update(1)

            # Save checkpoint
            if is_main and step % config['ckpt_interval'] == 0:
                # Remove old checkpoints with error handling
                ckpt_paths = sorted(glob(path.join(config['log_dir'], 'ckpt-*.pt')))
                for old_ckpt_path in ckpt_paths:
                    try:
                        os.remove(old_ckpt_path)
                    except OSError:
                        pass  # File may be in use, skip

                new_ckpt_path = path.join(config['log_dir'], f'ckpt-{step}.pt')
                
                # Get unwrapped model state dict
                unwrapped_model = accelerator.unwrap_model(model)
                
                torch.save({
                    'step': step,
                    'config': config,
                    'model': unwrapped_model.state_dict(),
                    'optimizer': optimizer.state_dict(),
                    'lr_sched': lr_sched.state_dict(),
                }, new_ckpt_path)
                print(f'\nCheckpoint saved to {new_ckpt_path}')

    # Cleanup
    if is_main:
        writer.flush()
    
    envs.close()

    if is_main:
        end_time = datetime.now()
        print(f'\nTraining ended at {end_time}')
        print(f'Elapsed time: {end_time - start_time}')
