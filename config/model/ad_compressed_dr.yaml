include: config/model/ad_base.yaml

model: CompressedAD

# Transformer Decoder - SIMPLIFIED FOR BETTER LEARNING
tf_n_embd: 64  # Smaller embedding for easier learning
tf_n_layer: 3  # Fewer layers
tf_n_head: 4  # Fewer heads
tf_dropout: 0.2  # Higher dropout to prevent overfitting
tf_attn_dropout: 0.2
tf_dim_feedforward: 256  # Smaller feedforward
decoder_max_seq_length: 200  # Reduced max length

# Compression Encoder - MUCH SIMPLER
encoder_n_layer: 2  # Fewer encoder layers
encoder_n_head: 4  # Match decoder heads
encoder_dim_feedforward: 256  # Match decoder feedforward
max_encoder_length: 400  # Max input length to encoder
n_latent: 20  # Fewer latent tokens for stronger compression signal

use_reconstruction_loss: true

# Pre-trained encoder settings (optional - uncomment to use pretrained encoder)
pretrained_encoder_path: './runs/pretrain-encoder-darkroom-seed0/encoder-pretrained-final.pt'
freeze_encoder: false  # true = frozen encoder, false = fine-tune encoder (recommended)

# Compression parameters for dataset - SIMPLIFIED
max_compression_depth: 2  # Reduce to 2 cycles max (0, 1, 2)
min_compress_length: 20  # Longer segments
max_compress_length: 40  # Shorter max to keep manageable
min_uncompressed_length: 15  # More uncompressed context
max_uncompressed_length: 80  # Reduced to fit in decoder_max - n_latent - 1

# Context window (for AD compatibility)
n_transit: 200  

# Training - OPTIMIZED FOR SIMPLER MODEL
lr: 0.0003  # Higher LR for smaller model (same as standard AD)
train_batch_size: 256  # Larger batch for stability
test_batch_size: 512
train_source_timesteps: 1000
train_timesteps: 100000  # Standard training length
num_warmup_steps: 2000  # Standard warmup
weight_decay: 0.01
grad_clip_norm: 1.0

num_workers: 4

