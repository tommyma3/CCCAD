include: config/model/ad_base.yaml

model: CompressedAD

# AD Transformer
tf_n_embd: 64
tf_n_layer: 4
tf_n_head: 4
tf_dropout: 0.1
tf_attn_dropout: 0.1
tf_n_inner: 128
n_transit: 240  

# Compression Settings
n_compress_tokens: 64   
compress_n_layers: 3    
compress_n_heads: 8     

# Gradient control - allow more gradient rounds for better learning
max_gradient_rounds: 3  # Increased from 2

# Reconstruction regularization during fine-tuning
use_recon_reg: True
recon_reg_weight: 0.05  # Reduced to let action loss dominate

# Curriculum settings (None = unlimited)
max_compressions: null

# Extended dataset settings for multi-compression training
# With n_transit=240, compressions occur roughly every 175 steps
# For 6 compressions we need ~1050+ context, for 8 compressions ~1400+ context
min_context_length: 50
max_context_length: 2000  # Increased from 800 to allow more compressions

# Extended length distribution - emphasize multi-compression scenarios
length_distribution:
  short: 0.10      # No compression: 50-200 (~10% of training)
  medium: 0.15     # 1 compression: 250-400 
  long: 0.25       # 2-3 compressions: 450-700
  very_long: 0.25  # 4-5 compressions: 750-1200
  extended: 0.25   # 6+ compressions: 1300-2000

# Training - longer training for better convergence
lr: 0.0002  # Slightly lower LR for stability with longer sequences
train_batch_size: 64  # Reduced batch size for memory with longer sequences
test_batch_size: 256
train_source_timesteps: 2500  # Increased from 1000 to support longer contexts
train_timesteps: 200000  # Longer training
num_warmup_steps: 2000   # More warmup

num_workers: 4

# Extended curriculum - more gradual progression
curriculum_schedule:
  - step: 0
    max_compressions: 1
  - step: 10000
    max_compressions: 2
  - step: 30000
    max_compressions: 4
  - step: 60000
    max_compressions: 6
  - step: 100000
    max_compressions: null  # Unlimited
