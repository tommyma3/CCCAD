include: config/model/ad_base.yaml

model: CompressedAD

# AD Transformer - smaller for computational efficiency
# With n_transit=60 and n_compress_tokens=16:
#   - Available space after compression: 60 - 16 - 1 = 43 tokens
#   - Compression triggers every ~43 new transitions
tf_n_embd: 64
tf_n_layer: 4
tf_n_head: 4
tf_dropout: 0.1
tf_attn_dropout: 0.1
tf_n_inner: 128         # 2x embedding dim
n_transit: 60           # Reduced from 240 - key computational saving

# Compression Settings - compact latent representation
n_compress_tokens: 16   # Reduced from 64 - more aggressive compression
compress_n_layers: 3
compress_n_heads: 4

# Gradient control
max_gradient_rounds: 2

# Reconstruction regularization
use_recon_reg: True
recon_reg_weight: 0.3

# Curriculum settings (None = unlimited)
max_compressions: null

# Dataset settings
# With n_transit=60, compression triggers at ~43 steps
# Length categories adjusted accordingly:
#   short: 20-50 (no compression needed)
#   medium: 60-150 (1-2 compressions)
#   long: 200-400 (3-8 compressions)
#   very_long: 500-1000 (10+ compressions)
min_context_length: 20
max_context_length: 1000

# Curriculum with stage-specific length distributions
curriculum_schedule:
  - step: 0
    max_compressions: 1
    # Focus on short/medium - learn basic AD and single compression
    length_distribution:
      short: 0.50       # No compression: 20-50
      medium: 0.45      # 1-2 compressions: 60-150
      long: 0.05        # Minimal long
      very_long: 0.00   # None
  - step: 25000
    max_compressions: 3
    # More compressions allowed
    length_distribution:
      short: 0.35
      medium: 0.40
      long: 0.20        # 3-8 compressions: 200-400
      very_long: 0.05
  - step: 60000
    max_compressions: 6
    # Balanced distribution
    length_distribution:
      short: 0.25
      medium: 0.30
      long: 0.30
      very_long: 0.15
  - step: 100000
    max_compressions: null  # Unlimited
    # Final distribution - all lengths
    length_distribution:
      short: 0.20
      medium: 0.25
      long: 0.30
      very_long: 0.25   # 10+ compressions: 500-1000

# Training
lr: 0.0003          # Can use higher LR for smaller model
train_batch_size: 128 # Can use larger batch with smaller model
test_batch_size: 256
train_source_timesteps: 1000
train_timesteps: 100000
num_warmup_steps: 1000

num_workers: 4
